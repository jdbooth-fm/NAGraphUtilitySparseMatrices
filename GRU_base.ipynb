{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt3a7U8_4pDW"
   },
   "source": [
    "#Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11621,
     "status": "ok",
     "timestamp": 1655750686240,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "9lnOSWHmDcu4"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
    "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cpu.html\n",
    "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
    "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cpu.html\n",
    "#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1655750686241,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "-38DxkiJDlQY"
   },
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5328,
     "status": "ok",
     "timestamp": 1655750691563,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "S7mJ1HXv4mNj",
    "outputId": "b4c36007-b121-4690-b3e1-9e8386c2a680"
   },
   "outputs": [],
   "source": [
    "#!pip install chompack\n",
    "\n",
    "import chompack as cp\n",
    "from cvxopt import spmatrix, amd, matrix, sparse\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TTOunMG492e"
   },
   "source": [
    "#Read in matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1655750691563,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "Nvyt7XZib_wx"
   },
   "outputs": [],
   "source": [
    "# This function returns the fill-in count for a matrix\n",
    "def getFillIn(mat):\n",
    "  # Convert the numpy mat to a cvxopt sparse matrix\n",
    "  spmat = sparse(matrix(mat.astype(np.double)))\n",
    "\n",
    "  # By default does not perform any permutations\n",
    "  symb = cp.symbolic(spmat)\n",
    "  fillIn = symb.fill[0]\n",
    "  fillIn = symb.nnz\n",
    "  #print(mat)\n",
    "  #print(symb)\n",
    "  #print(symb.fill[0], symb.fill[1], symb.nnz)\n",
    "\n",
    "  return int(fillIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1655750693757,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "BWL5G9B55Jtc",
    "outputId": "b8f033f2-df4f-444a-dc57-8ec1b8d0585d"
   },
   "outputs": [],
   "source": [
    "fpprec = 'float32'\n",
    "\n",
    "filename = '2_512_mat_nnz.mat'\n",
    "\n",
    "print('Max NNZ:', 128*128)\n",
    "\n",
    "loaded = scipy.io.loadmat(filename)\n",
    "#[data][0-flatten][order][0-flatten]  order: 0-rcm, 1-amd, 2-nd, 3-nat\n",
    "x_train_rcm = loaded['x_train'][0][0][0]\n",
    "y_train_rcm = loaded['y_train'][0][0][0]\n",
    "x_train_amd = loaded['x_train'][0][1][0]\n",
    "y_train_amd = loaded['y_train'][0][1][0]\n",
    "x_train_nd  = loaded['x_train'][0][2][0]\n",
    "y_train_nd  = loaded['y_train'][0][2][0]\n",
    "x_train     = loaded['x_train'][0][3][0]\n",
    "y_train     = loaded['y_train'][0][3][0]\n",
    "\n",
    "x_test_rcm = loaded['x_test'][0][0][0]\n",
    "y_test_rcm = loaded['y_test'][0][0][0]\n",
    "x_test_amd = loaded['x_test'][0][1][0]\n",
    "y_test_amd = loaded['y_test'][0][1][0]\n",
    "x_test_nd  = loaded['x_test'][0][2][0]\n",
    "y_test_nd  = loaded['y_test'][0][2][0]\n",
    "x_test     = loaded['x_test'][0][3][0]\n",
    "y_test     = loaded['y_test'][0][3][0]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(x_train_rcm)):\n",
    "    x_train_rcm[i] = x_train_rcm[i].astype('float32')\n",
    "for i in range(len(x_train_amd)):\n",
    "    x_train_amd[i] = x_train_amd[i].astype('float32')\n",
    "for i in range(len(x_train_nd)):\n",
    "    x_train_nd[i] = x_train_nd[i].astype('float32')\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = x_train[i].astype('float32')\n",
    "for i in range(len(x_test_rcm)):\n",
    "    x_test_rcm[i] = x_test_rcm[i].astype('float32')\n",
    "for i in range(len(x_test_amd)):\n",
    "    x_test_amd[i] = x_test_amd[i].astype('float32')\n",
    "for i in range(len(x_test_nd)):\n",
    "    x_test_nd[i] = x_test_nd[i].astype('float32')\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = x_test[i].astype('float32')\n",
    "    \n",
    "print(x_train_rcm)\n",
    "print(x_train_rcm[0].dtype)\n",
    "print(y_train_rcm[0].dtype)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1655750693758,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "97_DF4iCfjUA",
    "outputId": "77c41ef6-263e-4e13-ce50-0803382b97b1"
   },
   "outputs": [],
   "source": [
    "#Test order\n",
    "testi = 2\n",
    "print('NNZ. NAT: ', y_train[testi], ' RCM ', y_train_rcm[testi], ' AMD ', y_train_amd[testi], ' ND ', y_train_nd[testi])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup(x,y):\n",
    "    dup_counter = 0;\n",
    "    skip_list = [0]*len(x);\n",
    "    print('skip list len', len(x), len(skip_list))\n",
    "    new_x = [];\n",
    "    new_y = [];\n",
    "    for i in range(len(x)):\n",
    "        if(skip_list[i] == 0):\n",
    "            for j in range(i+1, len(x)):\n",
    "                if np.array_equal(x[i], x[j]):\n",
    "                    skip_list[j] = 1;\n",
    "                    dup_counter += 1\n",
    "            new_x.append(x[i]);\n",
    "            new_y.append(y[i]);\n",
    "            \n",
    "    print('Duplicates removed: ', dup_counter)\n",
    "    new_size = len(x) - dup_counter;\n",
    "    print('new size: ', new_size, len(new_x))\n",
    "    \n",
    "    return new_x, new_y\n",
    "    #return np.array(new_x), np.array(new_y)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rcm, y_train_rcm = remove_dup(x_train_rcm, y_train_rcm);\n",
    "x_train_amd, y_train_amd = remove_dup(x_train_amd, y_train_amd);\n",
    "x_train_nd, y_train_nd   = remove_dup(x_train_nd, y_train_nd);\n",
    "x_train, y_train         = remove_dup(x_train, y_train);\n",
    "x_test_rcm, y_test_rcm   = remove_dup(x_test_rcm, y_test_rcm);\n",
    "x_test_amd, y_test_amd   = remove_dup(x_test_amd, y_test_amd);\n",
    "x_test_nd,  y_test_nd    = remove_dup(x_test_nd, y_test_nd);\n",
    "x_test, y_test           = remove_dup(x_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(Atrain, ytrain, Atest, ytest):\n",
    "    ratio = .80\n",
    "    Atemp = Atrain + Atest;\n",
    "    ytemp = ytrain+ ytest;\n",
    "    Atrain_out = [];\n",
    "    ytrain_out = [];\n",
    "    Atest_out = [];\n",
    "    ytest_out = [];\n",
    "    print('length: ', len(Atemp), len(ytemp));\n",
    "    randp = np.random.permutation(len(ytemp));\n",
    "    \n",
    "    ratio = len(ytemp)*ratio\n",
    "    for i in range(len(ytemp)):\n",
    "        if(i <= ratio):\n",
    "            Atrain_out.append(Atemp[i]);\n",
    "            ytrain_out.append(ytemp[i]);\n",
    "        else:\n",
    "            Atest_out.append(Atemp[i]);\n",
    "            ytest_out.append(ytemp[i]);\n",
    "            \n",
    "    print('end length: ', len(ytrain_out), len(ytest_out) );\n",
    "    \n",
    "    \n",
    "    return np.array(Atrain_out), np.array(ytrain_out), np.array(Atest_out), np.array(ytest_out);\n",
    "    #return Atrain_out, ytrain_out, Atest_out, ytest_out;\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rcm, y_train_rcm, x_test_rcm, y_test_rcm = rebalance(x_train_rcm, y_train_rcm, x_test_rcm, y_test_rcm);\n",
    "x_train_amd, y_train_amd, x_test_amd, y_test_amd = rebalance(x_train_amd, y_train_amd, x_test_amd, y_test_amd);\n",
    "x_train_nd, y_train_nd, x_test_nd, y_test_nd     = rebalance(x_train_nd, y_train_nd, x_test_nd, y_test_nd);\n",
    "x_train, y_train, x_test, y_test                 = rebalance(x_train, y_train, x_test, y_test);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDq1YWqt-bEy"
   },
   "source": [
    "#Visualize the fill-in distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4154,
     "status": "ok",
     "timestamp": 1655750697908,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "L1-En_hN-p_T",
    "outputId": "bb40e78d-7da9-4667-8755-5b07687393d5"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of training and test set fill-in counts\n",
    "fig, ax = plt.subplots(1, figsize=(15,7))\n",
    "numBins = 100\n",
    "normalize = True\n",
    "\n",
    "# Distributions should appear about the same\n",
    "# Training set is in BLUE, testing set is in RED\n",
    "ax.hist(y_train_rcm, bins=numBins, density=normalize, \n",
    "        fc=(0, 0, 1, 0.5), label='training set')\n",
    "\n",
    "ax.hist(y_test_rcm,  bins=numBins, density=normalize, \n",
    "        fc=(1, 0, 0, 0.5), label='testing set')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Fill-in Count')\n",
    "ax.set_ylabel('Normalized Histogram Counts')\n",
    "ax.set_title('Training and Test Set Fill-In Histograms -- RCM')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of training and test set fill-in counts\n",
    "fig, ax = plt.subplots(1, figsize=(15,7))\n",
    "numBins = 100\n",
    "normalize = True\n",
    "\n",
    "# Distributions should appear about the same\n",
    "# Training set is in BLUE, testing set is in RED\n",
    "ax.hist(y_train_amd, bins=numBins, density=normalize, \n",
    "        fc=(0, 0, 1, 0.5), label='training set')\n",
    "\n",
    "ax.hist(y_test_amd,  bins=numBins, density=normalize, \n",
    "        fc=(1, 0, 0, 0.5), label='testing set')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Fill-in Count')\n",
    "ax.set_ylabel('Normalized Histogram Counts')\n",
    "ax.set_title('Training and Test Set Fill-In Histograms -- AMD')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of training and test set fill-in counts\n",
    "fig, ax = plt.subplots(1, figsize=(15,7))\n",
    "numBins = 100\n",
    "normalize = True\n",
    "\n",
    "# Distributions should appear about the same\n",
    "# Training set is in BLUE, testing set is in RED\n",
    "ax.hist(y_train_nd, bins=numBins, density=normalize, \n",
    "        fc=(0, 0, 1, 0.5), label='training set')\n",
    "\n",
    "ax.hist(y_test_nd,  bins=numBins, density=normalize, \n",
    "        fc=(1, 0, 0, 0.5), label='testing set')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Fill-in Count')\n",
    "ax.set_ylabel('Normalized Histogram Counts')\n",
    "ax.set_title('Training and Test Set Fill-In Histograms -- ND')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of training and test set fill-in counts\n",
    "fig, ax = plt.subplots(1, figsize=(15,7))\n",
    "numBins = 100\n",
    "normalize = True\n",
    "\n",
    "# Distributions should appear about the same\n",
    "# Training set is in BLUE, testing set is in RED\n",
    "ax.hist(y_train, bins=numBins, density=normalize, \n",
    "        fc=(0, 0, 1, 0.5), label='training set')\n",
    "\n",
    "ax.hist(y_test,  bins=numBins, density=normalize, \n",
    "        fc=(1, 0, 0, 0.5), label='testing set')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Fill-in Count')\n",
    "ax.set_ylabel('Normalized Histogram Counts')\n",
    "ax.set_title('Training and Test Set Fill-In Histograms -- Natural')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUdnBPmeDRNZ"
   },
   "source": [
    "#Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1655750698066,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "vJQGcLP-DVkR",
    "outputId": "bb6852fc-066d-48d0-b107-a82c9cff7782"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "#Build Graph\n",
    "#0->1, 0->2, 0->3, ...., 0->n (i.e., 0->127)\n",
    "#1->2, 1->3, ....., 1->n\n",
    "#etc\n",
    "N = 512;\n",
    "\n",
    "edge_index = torch.tensor([[N-2,N-1]], dtype=torch.long)\n",
    "for out_edge in range(N-2):\n",
    "  for in_edge in range(out_edge+1, N):\n",
    "    #print('out', out_edge, 'in', in_edge)\n",
    "    temp = torch.tensor([[out_edge, in_edge]], dtype=torch.long)\n",
    "    edge_index = torch.cat((edge_index,temp))\n",
    "    #print(edge_index)\n",
    "\n",
    "edge_index = edge_index.t().contiguous() #need transpose\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "\n",
    "print(x_train_rcm[0])\n",
    "\n",
    "data_rcm = Data(x=x_train_rcm[1], edge_index=edge_index, num_nodes=512, num_node_features=512)\n",
    "data_amd = Data(x=x_train_amd[1], edge_index=edge_index, num_nodes=512, num_node_features=512)\n",
    "data_nd  = Data(x=x_train_nd[1], edge_index=edge_index, num_nodes=512, num_node_features=512)\n",
    "data_nat  = Data(x=x_train[1], edge_index=edge_index, num_nodes=512, num_node_features=512)\n",
    "\n",
    "data_rcm = data_rcm.to(device)\n",
    "data_amd = data_amd.to(device)\n",
    "data_nd  = data_nd.to(device)\n",
    "data_nat = data_nat.to(device)\n",
    "\n",
    "print(data_rcm)\n",
    "print(data_amd)\n",
    "print(data_nd)\n",
    "print(data_nat)\n",
    "print('number of nodes', data_rcm.num_nodes)\n",
    "print('number of edges' , data_rcm.num_edges)\n",
    "print('number of features', data_rcm.num_node_features)\n",
    "#print(data.num_classes)\n",
    "print(data_rcm.has_isolated_nodes())\n",
    "print(data_rcm.is_directed())\n",
    "print(data_rcm.has_self_loops())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSM-emYvFQkP"
   },
   "source": [
    "#Make NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1655750698221,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "ea73lBpCFUJP"
   },
   "outputs": [],
   "source": [
    "h1_size = 32\n",
    "h2_size = 16\n",
    "\n",
    "class GModel(torch.nn.Module):\n",
    "  def __init__(self, data):\n",
    "    super().__init__()\n",
    "    self.layer1 = torch.nn.GRU(512,512,num_layers=1, batch_first=True, \n",
    "                              bias=False, dropout=.1, bidirectional=False)\n",
    "    self.layer3 = torch.nn.Linear(512,1, bias=False)\n",
    "    self.layer4 = torch.nn.Linear(512,1, bias=False)\n",
    "    \n",
    "    torch.nn.init.ones_(self.layer3.weight)\n",
    "    torch.nn.init.ones_(self.layer4.weight)\n",
    "    \n",
    "  def forward(self, x, data):\n",
    "    #edge_index =  data.edge_index\n",
    "    #print(x)\n",
    "    #print(edge_index)\n",
    "    #x = torch.reshape(x, (1,512*512))\n",
    "    x, hidden = self.layer1(x)\n",
    "    #print('after first', x.shape)\n",
    "    x = self.layer3(x)\n",
    "    #print('after two', x.shape)\n",
    "    x = self.layer4(x.t().contiguous())\n",
    "    \n",
    "    #print(x.shape)\n",
    "    #x = self.layer3(x)\n",
    "    #print(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOXWcitXFp-I"
   },
   "source": [
    "#Train Visual Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1655750698602,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "w-5laDXPFsu_"
   },
   "outputs": [],
   "source": [
    "from torch._C import set_num_interop_threads\n",
    "def check_acc(loader, model, data):\n",
    "  diff_fun = torch.nn.L1Loss()\n",
    "  min = 9999999;\n",
    "  max = 0;\n",
    "  max_fill = 0;\n",
    "  suml1 = 0\n",
    "  avgl1 = 0\n",
    "  ntotal = 0\n",
    "  model.eval();\n",
    "  with torch.no_grad():\n",
    "    for index,  (x,y) in enumerate(loader):\n",
    "      #print('index', index)\n",
    "      ntotal += 1\n",
    "      x = torch.squeeze(x)\n",
    "      y = torch.squeeze(y)\n",
    "      scores = model(x, data);\n",
    "      scores = scores.squeeze();\n",
    "      #print(scores.size())\n",
    "      if(index == 0 ):\n",
    "        #print('first')\n",
    "        y_val = torch.tensor([scores]);\n",
    "        y_val_test = torch.tensor([y]);\n",
    "      else:\n",
    "        #print('second')\n",
    "        #print('scores', scores.shape, 'yval', y_val.shape)\n",
    "        y_val = torch.cat((y_val,torch.tensor([scores])))\n",
    "        y_val_test = torch.cat((y_val_test,torch.tensor([y])))\n",
    "        #print(y_val, y_val_test)\n",
    "      #print(y_val.shape)\n",
    "      loss = diff_fun(scores,y)\n",
    "      loss_hand = torch.abs(torch.subtract(scores,y))\n",
    "      #print(loss_hand.item()\n",
    "      suml1 += loss_hand.item()\n",
    "      if loss < min:\n",
    "        min = loss;\n",
    "      if loss > max:\n",
    "        max = loss\n",
    "      #print(torch.max(y))\n",
    "      if torch.max(y).item() > max_fill:\n",
    "        max_fill = torch.max(y).item()\n",
    "      #print('loss', loss)\n",
    "\n",
    "  print('max:', max, 'min:', min, 'suml1', suml1, 'avg', suml1/ntotal)\n",
    "  model.train()\n",
    "\n",
    "\n",
    "  #print(y_val)\n",
    "  #print(y_val_test)\n",
    "\n",
    "  fig = plt.figure(figsize=(6,6));\n",
    "  plt.plot([0,max_fill+100], [0,max_fill+100], '--')\n",
    "  plt.scatter(y_val_test,y_val, c='blue');\n",
    "  plt.xlim(0,max_fill+100)\n",
    "  plt.ylim(0, max_fill+100)\n",
    "  plt.xlabel('True Fill-in')\n",
    "  plt.ylabel('Predicted Fill-in')\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYbLNLf6GYmA"
   },
   "source": [
    "#Build DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1655750698603,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "A7D-rJldGbPH"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    super(MyDataset,self).__init__()\n",
    "    self.x = x;\n",
    "    self.y = y;\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655750698603,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "C8LbX5QmGfgA"
   },
   "outputs": [],
   "source": [
    "#natural \n",
    "x_train = torch.tensor(x_train).to(device)\n",
    "y_train = torch.tensor(y_train).to(device)\n",
    "x_test  = torch.tensor(x_test).to(device)\n",
    "y_test  = torch.tensor(y_test).to(device)\n",
    "train_dataset_nat = MyDataset(x_train, y_train)\n",
    "train_loader_nat = DataLoader(dataset=train_dataset_nat, batch_size=1, shuffle=True)\n",
    "test_dataset_nat = MyDataset(x_test, y_test)\n",
    "test_loader_nat = DataLoader(dataset=test_dataset_nat, batch_size=1, shuffle=False)\n",
    "#rcm\n",
    "y_train_rcm = torch.tensor(y_train_rcm).to(device)\n",
    "y_test_rcm  = torch.tensor(y_test_rcm).to(device)\n",
    "train_dataset_rcm = MyDataset(x_train, y_train_rcm)\n",
    "train_loader_rcm = DataLoader(dataset=train_dataset_rcm, batch_size=1, shuffle=True)\n",
    "test_dataset_rcm = MyDataset(x_test, y_test_rcm)\n",
    "test_loader_rcm = DataLoader(dataset=test_dataset_rcm, batch_size=1, shuffle=False)\n",
    "#amd\n",
    "y_train_amd = torch.tensor(y_train_amd).to(device)\n",
    "y_test_amd  = torch.tensor(y_test_amd).to(device)\n",
    "train_dataset_amd = MyDataset(x_train, y_train_amd)\n",
    "train_loader_amd = DataLoader(dataset=train_dataset_amd, batch_size=1, shuffle=True)\n",
    "test_dataset_amd = MyDataset(x_test, y_test_amd)\n",
    "test_loader_amd = DataLoader(dataset=test_dataset_amd, batch_size=1, shuffle=False)\n",
    "#nd\n",
    "y_train_nd = torch.tensor(y_train_nd).to(device)\n",
    "y_test_nd  = torch.tensor(y_test_nd).to(device)\n",
    "train_dataset_nd = MyDataset(x_train, y_train_nd)\n",
    "train_loader_nd = DataLoader(dataset=train_dataset_nd, batch_size=1, shuffle=True)\n",
    "test_dataset_nd = MyDataset(x_test, y_test_nd)\n",
    "test_loader_nd = DataLoader(dataset=test_dataset_nd, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2PFBVDGcxqB"
   },
   "source": [
    "#Train Nat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Mh4_ttX8KJGVkulliuBEcM-4qMW7MP91"
    },
    "executionInfo": {
     "elapsed": 2516840,
     "status": "ok",
     "timestamp": 1655753215440,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "Q9QFaMZXc0pY",
    "outputId": "c4495850-e0ac-4000-c697-767b19dbf363"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nepoch = 200;\n",
    "\n",
    "net_nat = GModel(data_nat)\n",
    "net_nat = net_nat.to(device)\n",
    "print(net_nat)\n",
    "#xtemp = torch.from_numpy(x_train[0]);\n",
    "#print(xtemp)\n",
    "#output = net(xtemp, data)\n",
    "#print('output test', output)\n",
    "\n",
    "\n",
    "mycrit = torch.nn.SmoothL1Loss();\n",
    "#mycrit = torch.nn.MSELoss();\n",
    "#myoptimizer_nat = torch.optim.RMSprop(net_nat.parameters());\n",
    "myoptimizer_nat = torch.optim.Adagrad(net_nat.parameters());\n",
    "#myoptimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "net_nat.train()\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "  loss = 0;\n",
    "  for batch_idx, (xtemp, ytemp) in enumerate(train_loader_nat):\n",
    "   \n",
    "    #forward step\n",
    "    #print('one', xtemp)\n",
    "    xtemp = torch.squeeze(xtemp)\n",
    "    #print('two', xtemp)\n",
    "    scores = net_nat(xtemp, data_nat)\n",
    "    #remove extra tensor layers\n",
    "    scores = torch.squeeze(scores)\n",
    "    ytemp = torch.squeeze(ytemp)\n",
    "    loss = mycrit(scores, ytemp) \n",
    "\n",
    "    #back step\n",
    "    myoptimizer_nat.zero_grad();\n",
    "    loss.backward();\n",
    "    myoptimizer_nat.step();\n",
    "\n",
    "  check_acc(test_loader_nat,net_nat, data_nat)\n",
    "  print('epoch:', epoch, 'loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5LqvISDeAby"
   },
   "source": [
    "#Test Nat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "executionInfo": {
     "elapsed": 8708,
     "status": "ok",
     "timestamp": 1655753224131,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "582kaDqweCsg",
    "outputId": "b2fd265b-a807-43d9-d231-3edc4de50efd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#save\n",
    "#torch.save(net_nat.state_dict(), 'net_nat_gru.model')\n",
    "net_nat = GModel(data_nat)\n",
    "net_nat = net_nat.to(device)\n",
    "net_nat.load_state_dict(torch.load('net_nat_gru.model'))\n",
    "#check\n",
    "\n",
    "check_acc(train_loader_nat, net_nat, data_nat)\n",
    "check_acc(test_loader_nat, net_nat, data_nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vd36UClwHSJJ"
   },
   "source": [
    "#Train RCM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1t3HW8ixxDlszGxSY6L12k1hgnSZR-56K"
    },
    "executionInfo": {
     "elapsed": 2501866,
     "status": "ok",
     "timestamp": 1655755725985,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "XF2iG4MYHRdH",
    "outputId": "59601b53-df6d-4938-e30a-ee4f3f6ba840"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#nepoch = 100;\n",
    "\n",
    "net_rcm = GModel(data_rcm)\n",
    "net_rcm = net_rcm.to(device)\n",
    "print(net_rcm)\n",
    "#xtemp = torch.from_numpy(x_train[0]);\n",
    "#print(xtemp)\n",
    "#output = net(xtemp, data)\n",
    "#print('output test', output)\n",
    "\n",
    "\n",
    "mycrit = torch.nn.SmoothL1Loss();\n",
    "#mycrit = torch.nn.MSELoss();\n",
    "#myoptimizer_rcm = torch.optim.RMSprop(net_rcm.parameters());\n",
    "myoptimizer_rcm = torch.optim.Adagrad(net_rcm.parameters());\n",
    "#myoptimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "net_rcm.train()\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "  loss = 0;\n",
    "  for batch_idx, (xtemp, ytemp) in enumerate(train_loader_rcm):\n",
    "   \n",
    "    #forward step\n",
    "    #print('one', xtemp)\n",
    "    xtemp = torch.squeeze(xtemp)\n",
    "    #print('two', xtemp)\n",
    "    scores = net_rcm(xtemp, data_rcm)\n",
    "    #remove extra tensor layers\n",
    "    scores = torch.squeeze(scores)\n",
    "    ytemp = torch.squeeze(ytemp)\n",
    "    loss = mycrit(scores, ytemp) \n",
    "\n",
    "    #back step\n",
    "    myoptimizer_rcm.zero_grad();\n",
    "    loss.backward();\n",
    "    myoptimizer_rcm.step();\n",
    "\n",
    "  check_acc(test_loader_rcm,net_rcm, data_rcm)\n",
    "  print('epoch:', epoch, 'loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HHytKGlMSP-"
   },
   "source": [
    "#Test RCM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 15007,
     "status": "ok",
     "timestamp": 1655755740989,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "Yqtn73sTMYwM",
    "outputId": "13797f80-5b6c-482c-817c-b9ab1ba58759"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#save\n",
    "#torch.save(net_rcm.state_dict(), 'net_rcm_gru.model')\n",
    "net_rcm = GModel(data_rcm)\n",
    "net_rcm = net_rcm.to(device)\n",
    "net_rcm.load_state_dict(torch.load('net_rcm_gru.model'))\n",
    "#check\n",
    "\n",
    "check_acc(train_loader_rcm, net_rcm, data_rcm)\n",
    "check_acc(test_loader_rcm, net_rcm, data_rcm)\n",
    "\n",
    "#check_acc(train_loader_nat, net_nat, data_nat)\n",
    "#check_acc(test_loader_nat, net_nat, data_nat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6cotYjHMVOt"
   },
   "source": [
    "#Train AMD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1j1Rbx75XGNodtfW_MdiioDQY_2wSu69C"
    },
    "executionInfo": {
     "elapsed": 2314977,
     "status": "ok",
     "timestamp": 1655758273851,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "3rysQ6R5MsFP",
    "outputId": "ad2f6fb4-f2d4-412a-9b99-98cdd8754aee"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nepoch = 200;\n",
    "\n",
    "net_amd = GModel(data_amd)\n",
    "net_amd = net_amd.to(device)\n",
    "print(net_amd)\n",
    "#xtemp = torch.from_numpy(x_train[0]);\n",
    "#print(xtemp)\n",
    "#output = net(xtemp, data)\n",
    "#print('output test', output)\n",
    "\n",
    "\n",
    "#mycrit = torch.nn.MSELoss();\n",
    "mycrit = torch.nn.SmoothL1Loss();\n",
    "#myoptimizer_amd = torch.optim.RMSprop(net_amd.parameters());\n",
    "myoptimizer_amd = torch.optim.Adagrad(net_amd.parameters());\n",
    "#myoptimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "net_amd.train()\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "  loss = 0;\n",
    "  for batch_idx, (xtemp, ytemp) in enumerate(train_loader_amd):\n",
    "   \n",
    "    #forward step\n",
    "    #print('one', xtemp)\n",
    "    xtemp = torch.squeeze(xtemp)\n",
    "    #print('two', xtemp)\n",
    "    scores = net_amd(xtemp, data_amd)\n",
    "    #remove extra tensor layers\n",
    "    scores = torch.squeeze(scores)\n",
    "    ytemp = torch.squeeze(ytemp)\n",
    "    loss = mycrit(scores, ytemp) \n",
    "\n",
    "    #back step\n",
    "    myoptimizer_amd.zero_grad();\n",
    "    loss.backward();\n",
    "    myoptimizer_amd.step();\n",
    "\n",
    "  check_acc(test_loader_amd,net_amd, data_amd)\n",
    "  print('epoch:', epoch, 'loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sElhj8fXNjAO"
   },
   "source": [
    "#Test AMD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "executionInfo": {
     "elapsed": 5998,
     "status": "ok",
     "timestamp": 1655758279901,
     "user": {
      "displayName": "Joshua Booth",
      "userId": "16242936417585954759"
     },
     "user_tz": 300
    },
    "id": "eJkF8K4lNnru"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#save\n",
    "torch.save(net_amd.state_dict(), 'net_amd_gru.model')\n",
    "#net_amd = GModel(data_amd)\n",
    "#net_amd.load_state_dict(torch.load('net_amd_smooth_ada_200_512.model'))\n",
    "#check\n",
    "check_acc(train_loader_amd, net_amd, data_amd)\n",
    "check_acc(test_loader_amd, net_amd, data_amd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70sa8N-FN32t"
   },
   "source": [
    "#Train ND Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1GVB0XKGjGYj4sqZVo4-8H2CAvm7yxDd_"
    },
    "id": "TNPL2JBwN0x9",
    "outputId": "c81754e2-a0cc-4b34-b458-b1bff5b036bd"
   },
   "outputs": [],
   "source": [
    "#nepoch = 100;\n",
    "\n",
    "net_nd = GModel(data_nd)\n",
    "net_nd = net_nd.to(device)\n",
    "print(net_nd)\n",
    "#xtemp = torch.from_numpy(x_train[0]);\n",
    "#print(xtemp)\n",
    "#output = net(xtemp, data)\n",
    "#print('output test', output)\n",
    "\n",
    "#mycrit = torch.nn.MSELoss();\n",
    "mycrit = torch.nn.SmoothL1Loss();\n",
    "#myoptimizer_nd = torch.optim.RMSprop(net_nd.parameters());\n",
    "myoptimizer_nd = torch.optim.Adagrad(net_nd.parameters());\n",
    "#myoptimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "net_nd.train()\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "  loss = 0;\n",
    "  for batch_idx, (xtemp, ytemp) in enumerate(train_loader_nd):\n",
    "   \n",
    "    #forward step\n",
    "    #print('one', xtemp)\n",
    "    xtemp = torch.squeeze(xtemp)\n",
    "    #print('two', xtemp)\n",
    "    scores = net_nd(xtemp, data_nd)\n",
    "    #remove extra tensor layers\n",
    "    scores = torch.squeeze(scores)\n",
    "    ytemp = torch.squeeze(ytemp)\n",
    "    loss = mycrit(scores, ytemp) \n",
    "\n",
    "    #back step\n",
    "    myoptimizer_nd.zero_grad();\n",
    "    loss.backward();\n",
    "    myoptimizer_nd.step();\n",
    "\n",
    "  check_acc(test_loader_nd,net_nd, data_nd)\n",
    "  print('epoch:', epoch, 'loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOWZn3xaO_89"
   },
   "source": [
    "#Test ND Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5HOUHE13PDeL"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#save\n",
    "torch.save(net_nd.state_dict(), 'net_nd_gru.model')\n",
    "#net_nd = GModel(data_nd)\n",
    "#net_nd.load_state_dict(torch.load('net_nd_smooth_ada_200_512.model'))\n",
    "#check\n",
    "check_acc(train_loader_nd, net_nd, data_nd)\n",
    "check_acc(test_loader_nd, net_nd, data_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qyD0w7XvVuy"
   },
   "source": [
    "#Visual of Weights Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7jEOgzXFvVDU"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def visual_linear_weights(weights):\n",
    "  print(weights)\n",
    "  min = torch.min(weights)\n",
    "  max = torch.max(weights)\n",
    "  print('min', min, 'max', max)\n",
    "  bound = torch.maximum(torch.abs(min), torch.abs(max))\n",
    "  print('bound', bound)\n",
    "  vmin = -bound\n",
    "  vmax = bound\n",
    "  epsilon = .01 if 0.01 <= vmax and .01 >= vmin else vmax/10\n",
    "  low0 = (-epsilon - vmin)/(vmax - vmin)\n",
    "  high0 = (epsilon + vmax)/(vmax - vmin)\n",
    "  print('low', low0, 'high', high0)\n",
    "\n",
    "  nodes = [0, low0, low0, high0, high0, 1]\n",
    "  colors = ['green', 'blue', 'black', 'black', 'red', 'white']\n",
    "  cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", list(zip(nodes, colors)))\n",
    "\n",
    "def heat_map(weights):\n",
    "  weights = weights.detach().numpy()\n",
    "  fig = plt.figure(figsize=(15,1));\n",
    "  #plt.imshow(weights, cmap='hot')\n",
    "  ax = sns.heatmap(weights, center=0)\n",
    "  plt.show()\n",
    "\n",
    "def heat_map2D(weights):\n",
    "  weights = weights.detach().numpy()\n",
    "  fig = plt.figure(figsize=(15,15));\n",
    "  #plt.imshow(weights, cmap='hot')\n",
    "  ax = sns.heatmap(weights, center=0)\n",
    "  plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nnz(weights):\n",
    "    weights = weights.detach().numpy()\n",
    "    m,n = weights.shape\n",
    "    #print(m,n)\n",
    "    counter = 0\n",
    "    eps = .01\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            val = weights[i][j]\n",
    "            if(abs(val)<eps):\n",
    "                counter += 1\n",
    "                \n",
    "    print('nnz', counter)\n",
    "    print('percentage', (counter/(m*n)*100.0))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbrP_nT_vecr"
   },
   "source": [
    "#Visual of Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ENtYSf5QvmYk"
   },
   "outputs": [],
   "source": [
    "#nat order\n",
    "print('NAT ORDER')\n",
    "heat_map(net_nat.layer3.weight.cpu())\n",
    "heat_map(net_nat.layer4.weight.cpu())\n",
    "heat_map2D(net_nat.layer1.weight.squeeze().t().cpu())\n",
    "heat_map2D(net_nat.layer2.weight.squeeze().t().cpu())\n",
    "#rcm order\n",
    "print('RCM ORDER')\n",
    "heat_map(net_rcm.layer3.weight.cpu())\n",
    "heat_map(net_rcm.layer4.weight.cpu())\n",
    "heat_map2D(net_rcm.layer1.weight.squeeze().t().cpu())\n",
    "heat_map2D(net_rcm.layer2.weight.squeeze().t().cpu())\n",
    "#amd order\n",
    "print('AMD ORDER')\n",
    "heat_map(net_amd.layer3.weight.cpu())\n",
    "heat_map(net_amd.layer4.weight.cpu())\n",
    "heat_map2D(net_amd.layer1.weight.squeeze().t().cpu())\n",
    "heat_map2D(net_amd.layer2.weight.squeeze().t().cpu())\n",
    "#nd order\n",
    "print('ND ORDER')\n",
    "heat_map(net_nd.layer3.weight.cpu())\n",
    "heat_map(net_nd.layer4.weight.cpu())\n",
    "heat_map2D(net_nd.layer1.weight.squeeze().t().cpu())\n",
    "heat_map2D(net_nd.layer2.weight.squeeze().t().cpu())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeLIM3gb3Utp"
   },
   "source": [
    "#Prediction Model Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4Qe43yUZ3g4a"
   },
   "outputs": [],
   "source": [
    "def PredictNNZ(loader_array):\n",
    "  #loaders hold the information\n",
    "  #data hold the parameter meta\n",
    "  eps = 10;\n",
    "\n",
    "  #Nat Loader\n",
    "  loader = loader_array[0]\n",
    "  loader_data = loader.dataset\n",
    "\n",
    "  #put models into eval mode\n",
    "  net_nat.eval()\n",
    "  net_rcm.eval()\n",
    "  net_amd.eval()\n",
    "  net_nd.eval()\n",
    "  \n",
    "  #variables\n",
    "  ntotal = 0\n",
    "  ncorrect = 0\n",
    "  nnotcorrect = 0\n",
    "  avg_diff = 0\n",
    "  percent_avg_diff = 0\n",
    "  with torch.no_grad():\n",
    "    for index in range(len(loader_data)):\n",
    "      #print('index', index)\n",
    "      ntotal += 1\n",
    "      [x,y] = loader_data[index]\n",
    "      #x = torch.tensor(x)\n",
    "\n",
    "\n",
    "      acc_score = []\n",
    "      acc_score_min_l = []\n",
    "      acc_order = []\n",
    "      #print('start value', acc_min_score)\n",
    "      for l_index in range(len(loader_array)):\n",
    "        temp_loader = loader_array[l_index]\n",
    "        [tempx_loader, tempy_loader] = temp_loader.dataset[index]\n",
    "        temp_v = tempy_loader\n",
    "        acc_score.append(temp_v)\n",
    "      acc_score_min = min(acc_score)\n",
    "      eps = acc_score_min*.1\n",
    "      for l_index in range(len(loader_array)):\n",
    "        if((acc_score[l_index] <= acc_score_min +eps) and (acc_score[l_index] >= acc_score_min - eps)):\n",
    "          acc_order.append(l_index)\n",
    "      #print(acc_order)\n",
    "      #print('Min: ', acc_score_min - eps, ' ', acc_score_min, ' ', acc_score_min + eps)\n",
    "      #print('Order: ', acc_score)\n",
    "\n",
    "        #print('Order: ', l_index, ' NNZ ', temp_v)\n",
    "        #temp_v = loader_array[l_index][index][1]\n",
    "        #temp_v = torch.squeeze(temp_v)\n",
    "        #if(temp_v < acc_min_score):\n",
    "          #acc_min_score = temp_v\n",
    "          #acc_order = l_index\n",
    "          #print('Index: ', index, ' Min: ', acc_min_score, ' Order: ', l_index)\n",
    "\n",
    "\n",
    "      scores_nat = net_nat(x, data_nat).squeeze().item()\n",
    "      scores_rcm = net_rcm(x, data_rcm).squeeze().item()\n",
    "      scores_amd = net_amd(x, data_rcm).squeeze().item()\n",
    "      scores_nd  = net_nd(x, data_nd).squeeze().item()\n",
    "      #print('Predicted NNZ NAT: ', scores_nat, ' RCM: ', scores_rcm, ' AMD: ', scores_amd, ' ND: ', scores_nd)\n",
    "      pred_min_score = scores_nat\n",
    "      pred_order = 0\n",
    "      if(scores_rcm < pred_min_score):\n",
    "        pred_min_score = scores_rcm\n",
    "        pred_order = 1\n",
    "      if(scores_amd < pred_min_score):\n",
    "        pred_min_score = scores_amd\n",
    "        pred_order = 2\n",
    "      if(scores_nd < pred_min_score):\n",
    "        pred_min_score = scores_nd\n",
    "        pred_order = 3\n",
    "\n",
    "        \n",
    "      nnz_diff = abs(pred_min_score - acc_score_min)\n",
    "      avg_diff += nnz_diff\n",
    "      percent_avg_diff += ((nnz_diff/acc_score_min)*100)\n",
    "      #print('Index: ', index, ' Acc NNZ: ', acc_min_score, ' Acc Order: ', acc_order, ' Pred NNZ: ', pred_min_score, ' Pred Order: ', pred_order )\n",
    "      if(pred_order in acc_order):\n",
    "        ncorrect +=1\n",
    "        #print('Correct')\n",
    "      else:\n",
    "        nnotcorrect +=1\n",
    "        #print('Not  Corect')\n",
    "      #print('Index: ', index, ' Acc Order: ', acc_order, ' Pred NNZ: ', pred_min_score, ' Pred Order: ', pred_order )\n",
    "\n",
    " \n",
    "\n",
    "  #print('Total: ', ntotal, ' Correct: ', ncorrect, ' Not Correct: ', nnotcorrect, ' Percent Correct ', ncorrect/ntotal)\n",
    "  #print('Avg Diff: ', avg_diff/ntotal, ' Percent Avg Diff: ', percent_avg_diff/ntotal)  \n",
    "    \n",
    "  return ntotal, ncorrect, nnotcorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9C7vRY5K17a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_loader_nat.shuffle = False; train_loader_rcm.shuffle = False; train_loader_amd.shuffle = False; train_loader_nd.shuffle = False;\n",
    "loader_array_train = [train_loader_nat, train_loader_rcm, train_loader_amd, train_loader_nd]\n",
    "%time PredictNNZ(loader_array_train)\n",
    "test_loader_nat.shuffle = False; test_loader_rcm.shuffle = False; test_loader_amd.shuffle = False; test_loader_nd.shuffle = False;\n",
    "loader_array_test = [test_loader_nat, test_loader_rcm, test_loader_amd, test_loader_nd]\n",
    "%time PredictNNZ(loader_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictNNZ_More(loader_array):\n",
    "  #loaders hold the information\n",
    "  #data hold the parameter meta\n",
    "  eps = 10;\n",
    "\n",
    "  #Nat Loader\n",
    "  loader = loader_array[0]\n",
    "  loader_data = loader.dataset\n",
    "\n",
    "  #put models into eval mode\n",
    "  net_nat.eval()\n",
    "  net_rcm.eval()\n",
    "  net_amd.eval()\n",
    "  net_nd.eval()\n",
    "  \n",
    "  #totals --- more for evaluation table\n",
    "  nat_obs = [0]*4;\n",
    "  rcm_obs = [0]*4; \n",
    "  amd_obs = [0]*4;\n",
    "  nd_obs  = [0]*4;\n",
    "\n",
    "  obs_dist = [0]*4;\n",
    "\n",
    "\n",
    "    \n",
    "  #variables\n",
    "  ntotal = 0\n",
    "  ncorrect = 0\n",
    "  nnotcorrect = 0\n",
    "  avg_diff = 0\n",
    "  percent_avg_diff = 0\n",
    "  with torch.no_grad():\n",
    "    for index in range(len(loader_data)):\n",
    "      #print('index', index)\n",
    "      ntotal += 1\n",
    "      [x,y] = loader_data[index]\n",
    "      #x = torch.tensor(x)\n",
    "\n",
    "\n",
    "      acc_score = []\n",
    "      acc_score_min_l = []\n",
    "      acc_order = []\n",
    "      #print('start value', acc_min_score)\n",
    "      for l_index in range(len(loader_array)):\n",
    "        temp_loader = loader_array[l_index]\n",
    "        [tempx_loader, tempy_loader] = temp_loader.dataset[index]\n",
    "        temp_v = tempy_loader\n",
    "        acc_score.append(temp_v)\n",
    "      acc_score_min = min(acc_score)\n",
    "    \n",
    "      obs_local_min = [];\n",
    "      for l_index in range(len(loader_array)):\n",
    "            if (acc_score[l_index] == acc_score_min):\n",
    "                obs_local_min.append(l_index);\n",
    "                obs_dist[l_index] = obs_dist[l_index]+1;\n",
    "                \n",
    "    \n",
    "    \n",
    "      eps = acc_score_min*.20\n",
    "      for l_index in range(len(loader_array)):\n",
    "        if((acc_score[l_index] <= acc_score_min +eps) and (acc_score[l_index] >= acc_score_min - eps)):\n",
    "          acc_order.append(l_index)\n",
    "      #print(acc_order)\n",
    "      #print('Min: ', acc_score_min - eps, ' ', acc_score_min, ' ', acc_score_min + eps)\n",
    "      #print('Order: ', acc_score)\n",
    "\n",
    "        #print('Order: ', l_index, ' NNZ ', temp_v)\n",
    "        #temp_v = loader_array[l_index][index][1]\n",
    "        #temp_v = torch.squeeze(temp_v)\n",
    "        #if(temp_v < acc_min_score):\n",
    "          #acc_min_score = temp_v\n",
    "          #acc_order = l_index\n",
    "          #print('Index: ', index, ' Min: ', acc_min_score, ' Order: ', l_index)\n",
    "\n",
    "\n",
    "      scores_nat = net_nat(x, data_nat).squeeze().item()\n",
    "      scores_rcm = net_rcm(x, data_rcm).squeeze().item()\n",
    "      scores_amd = net_amd(x, data_rcm).squeeze().item()\n",
    "      scores_nd  = net_nd(x, data_nd).squeeze().item()\n",
    "      #print('Predicted NNZ NAT: ', scores_nat, ' RCM: ', scores_rcm, ' AMD: ', scores_amd, ' ND: ', scores_nd)\n",
    "      pred_min_score = scores_nat\n",
    "      pred_order = 0\n",
    "      if(scores_rcm < pred_min_score):\n",
    "        pred_min_score = scores_rcm\n",
    "        pred_order = 1\n",
    "      if(scores_amd < pred_min_score):\n",
    "        pred_min_score = scores_amd\n",
    "        pred_order = 2\n",
    "      if(scores_nd < pred_min_score):\n",
    "        pred_min_score = scores_nd\n",
    "        pred_order = 3\n",
    "\n",
    "      #add more stuff\n",
    "      #if(0 in obs_local_min): #NAT observed\n",
    "      #      nat_obs[pred_order] = nat_obs[pred_order] + 1;\n",
    "      #if(1 in obs_local_min): #RCM observed\n",
    "      #      rcm_obs[pred_order] = rcm_obs[pred_order] + 1;\n",
    "      #if(2 in obs_local_min): #AMD observed\n",
    "      #      amd_obs[pred_order] = amd_obs[pred_order] + 1;\n",
    "      #if(3 in obs_local_min): #ND observed\n",
    "      #      nd_obs[pred_order] = nd_obs[pred_order] + 1;\n",
    "            \n",
    "            \n",
    "      if(pred_order in obs_local_min):\n",
    "        if(0 in obs_local_min):\n",
    "            nat_obs[0] = nat_obs[0]+1;\n",
    "        if(1 in obs_local_min):\n",
    "            rcm_obs[1] = rcm_obs[1]+1;\n",
    "        if (2 in obs_local_min):\n",
    "            amd_obs[2] = amd_obs[2]+1;\n",
    "        if (3 in obs_local_min):\n",
    "            nd_obs[3] = nd_obs[3] + 1;\n",
    "      else:\n",
    "        if(0 in obs_local_min):\n",
    "            nat_obs[pred_order] = nat_obs[pred_order]+1;\n",
    "        if(1 in obs_local_min):\n",
    "            rcm_obs[pred_order] = rcm_obs[pred_order]+1;\n",
    "        if (2 in obs_local_min):\n",
    "            amd_obs[pred_order] = amd_obs[pred_order]+1;\n",
    "        if (3 in obs_local_min):\n",
    "            nd_obs[pred_order] = nd_obs[pred_order] + 1;\n",
    "            \n",
    "        \n",
    "        \n",
    "      nnz_diff = abs(pred_min_score - acc_score_min)\n",
    "      avg_diff += nnz_diff\n",
    "      percent_avg_diff += ((nnz_diff/acc_score_min)*100)\n",
    "      #print('Index: ', index, ' Acc NNZ: ', acc_min_score, ' Acc Order: ', acc_order, ' Pred NNZ: ', pred_min_score, ' Pred Order: ', pred_order )\n",
    "      if(pred_order in acc_order):\n",
    "        ncorrect +=1\n",
    "        #print('Correct')\n",
    "      else:\n",
    "        nnotcorrect +=1\n",
    "        #print('Not  Corect')\n",
    "      #print('Index: ', index, ' Acc Order: ', acc_order, ' Pred NNZ: ', pred_min_score, ' Pred Order: ', pred_order )\n",
    "\n",
    "\n",
    "  print('Eval Matrix')\n",
    "  print('NAT:', end='')\n",
    "  for kk in range(4):\n",
    "    print(nat_obs[kk], ' , ', end='')\n",
    "  print(\"\")\n",
    "  print(\"RCM:\", end='')\n",
    "  for kk in range(4):\n",
    "    print(rcm_obs[kk], ', ', end='')\n",
    "  print(\"\")\n",
    "  print(\"AMD:\", end='')\n",
    "  for kk in range(4):\n",
    "    print(amd_obs[kk], ', ', end='')\n",
    "  print(\"\")\n",
    "  print(\"ND: \", end='')\n",
    "  for kk in range(4):\n",
    "    print(nd_obs[kk], ', ', end='')\n",
    "  print(\"\")\n",
    "\n",
    "\n",
    " \n",
    "  print('NAT: ', nat_obs[0], rcm_obs[0], amd_obs[0], nd_obs[0], (nat_obs[0])/(nat_obs[0]+rcm_obs[0]+amd_obs[0]+nd_obs[0]))\n",
    "  print('RCM: ', nat_obs[1], rcm_obs[1], amd_obs[1], nd_obs[1], (rcm_obs[1])/(nat_obs[1]+rcm_obs[1]+amd_obs[1]+nd_obs[1]))\n",
    "  print('AMD: ', nat_obs[2], rcm_obs[2], amd_obs[2], nd_obs[2], (amd_obs[2])/(nat_obs[2]+rcm_obs[2]+amd_obs[2]+nd_obs[2]))\n",
    "  print('ND: ', nat_obs[3], rcm_obs[3], amd_obs[3], nd_obs[3], (nd_obs[3])/(nat_obs[3]+rcm_obs[3]+amd_obs[3]+nd_obs[3]))\n",
    "  print('     ', nat_obs[0]/sum(nat_obs), rcm_obs[1]/sum(rcm_obs), amd_obs[2]/sum(amd_obs), nd_obs[3]/sum(nd_obs))\n",
    "\n",
    "\n",
    "  print('Distribution')\n",
    "  print(obs_dist)\n",
    "    \n",
    "\n",
    "  print('Total: ', ntotal, ' Correct: ', ncorrect, ' Not Correct: ', nnotcorrect, ' Percent Correct ', ncorrect/ntotal)\n",
    "  print('Avg Diff: ', avg_diff/ntotal, ' Percent Avg Diff: ', percent_avg_diff/ntotal)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_array_train = [train_loader_nat, train_loader_rcm, train_loader_amd, train_loader_nd]\n",
    "PredictNNZ_More(loader_array_train)\n",
    "\n",
    "loader_array_test = [test_loader_nat, test_loader_rcm, test_loader_amd, test_loader_nd]\n",
    "PredictNNZ_More(loader_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhcXmU4XjOTR/fvLULj38N",
   "collapsed_sections": [],
   "name": "MultipleOrders_TrainA_Smooth_AdaGrad.ipynb",
   "provenance": [
    {
     "file_id": "10JMNwD2dj0eGn3hU7t2QK0h4wMuAOMZo",
     "timestamp": 1655397808806
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
